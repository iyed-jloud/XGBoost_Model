{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        ".Installing XGBoost"
      ],
      "metadata": {
        "id": "yMBlXjV6TqH-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install xgboost"
      ],
      "metadata": {
        "id": "_obU8bD8SJAv",
        "outputId": "553ece47-f23a-4b24-da03-1c9119854f69",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.12/dist-packages (3.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from xgboost) (2.0.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.12/dist-packages (from xgboost) (2.29.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from xgboost) (1.16.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ".Basic XGBoost Model"
      ],
      "metadata": {
        "id": "jWi8IaAKUB3R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Create an instance of XGBRegressor\n",
        "xgb_model = xgb.XGBRegressor()\n",
        "\n",
        "# Fit the model on the training data\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = xgb_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model's performance\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Mean Squared Error:\", mse)"
      ],
      "metadata": {
        "id": "lsE_Ugw1SaMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ".Tuning Hyperparameters"
      ],
      "metadata": {
        "id": "vjqQVAYWUL2X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define the hyperparameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [3, 4, 5],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'subsample': [0.5, 0.8, 1],\n",
        "    'colsample_bytree': [0.5, 0.8, 1]\n",
        "}\n",
        "\n",
        "# Create an instance of XGBRegressor\n",
        "xgb_model = xgb.XGBRegressor()\n",
        "\n",
        "# Create the grid search object\n",
        "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')\n",
        "\n",
        "# Fit the grid search object to the data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best combination of hyperparameters\n",
        "best_params = grid_search.best_params_\n",
        "print(\"Best hyperparameters:\", best_params)\n",
        "\n",
        "# Train the model with the best hyperparameters\n",
        "best_xgb_model = xgb.XGBRegressor(**best_params)\n",
        "best_xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = best_xgb_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model's performance\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Mean Squared Error:\", mse)from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define the hyperparameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [3, 4, 5],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'subsample': [0.5, 0.8, 1],\n",
        "    'colsample_bytree': [0.5, 0.8, 1]\n",
        "}\n",
        "\n",
        "# Create an instance of XGBRegressor\n",
        "xgb_model = xgb.XGBRegressor()\n",
        "\n",
        "# Create the grid search object\n",
        "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')\n",
        "\n",
        "# Fit the grid search object to the data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best combination of hyperparameters\n",
        "best_params = grid_search.best_params_\n",
        "print(\"Best hyperparameters:\", best_params)\n",
        "\n",
        "# Train the model with the best hyperparameters\n",
        "best_xgb_model = xgb.XGBRegressor(**best_params)\n",
        "best_xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = best_xgb_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model's performance\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Mean Squared Error:\", mse)"
      ],
      "metadata": {
        "id": "y6baE-aiSevE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ".Advanced Techniques"
      ],
      "metadata": {
        "id": "gg9-u2R3UTQD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Early Stopping\n"
      ],
      "metadata": {
        "id": "r0Z6yfpEUWyc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the training data into train and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Fit the model with early stopping\n",
        "xgb_model = xgb.XGBRegressor()\n",
        "xgb_model.fit(X_train, y_train, early_stopping_rounds=10, eval_set=[(X_val, y_val)])"
      ],
      "metadata": {
        "id": "xTJ5p3sDSmA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Custom Loss Function"
      ],
      "metadata": {
        "id": "O60UFIIkUcfX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define custom objective function (example: Huber Loss)\n",
        "def huber_approx_obj(y_true, y_pred):\n",
        "    d = y_pred - y_true\n",
        "    scale = 1 + (d / 2)\n",
        "    squared_loss = np.square(d) / 2\n",
        "    linear_loss = np.abs(d) - 0.5\n",
        "    return np.where(np.abs(d) < 1, squared_loss, linear_loss) * scale\n",
        "\n",
        "# Fit the model with custom objective function\n",
        "xgb_model = xgb.XGBRegressor(obj=huber_approx_obj)\n",
        "xgb_model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "jFxM0IZMSpr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Importance"
      ],
      "metadata": {
        "id": "GLsNGI9TUf4J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model\n",
        "xgb_model = xgb.XGBRegressor()\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importances\n",
        "feature_importances = xgb_model.feature_importances_\n",
        "\n",
        "# Display feature importances\n",
        "for i, importance in enumerate(feature_importances):\n",
        "    print(f\"Feature {i}: {importance}\")\n",
        "\n",
        "# Select the most important features (example: top 5)\n",
        "top_n = 5\n",
        "most_important_indices = np.argsort(feature_importances)[-top_n:]\n",
        "X_train_selected = X_train[:, most_important_indices]\n",
        "X_test_selected = X_test[:, most_important_indices]"
      ],
      "metadata": {
        "id": "2PXPhERUSs6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regularization"
      ],
      "metadata": {
        "id": "gtDHGkIuUjKL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model with L1 and L2 regularization (alpha and lambda)\n",
        "xgb_model = xgb.XGBRegressor(alpha=1, lambda=1)\n",
        "xgb_model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "3FwsFXI3SwZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stacking"
      ],
      "metadata": {
        "id": "B_pm0_DGUmRK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor, StackingRegressor\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "# Create estimators for stacking\n",
        "estimators = [\n",
        "    ('xgb', xgb.XGBRegressor()),\n",
        "    ('rf', RandomForestRegressor())\n",
        "]\n",
        "\n",
        "# Create the stacking regressor\n",
        "stacking_regressor = StackingRegressor(estimators=estimators, final_estimator=Ridge())\n",
        "\n",
        "# Fit the stacking regressor\n",
        "stacking_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = stacking_regressor.predict(X_test)\n",
        "\n",
        "# Evaluate the model's performance\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Mean Squared Error:\", mse)"
      ],
      "metadata": {
        "id": "uF8KYucTSzh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ".What kind of problems can XGBoost solve?"
      ],
      "metadata": {
        "id": "nQYsY9mkUrLl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regression"
      ],
      "metadata": {
        "id": "h2LkfcwUUuSt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import xgboost as xgb\n",
        "\n",
        "# Load the dataset\n",
        "boston = load_boston()\n",
        "X = boston.data\n",
        "y = boston.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create an instance of XGBRegressor\n",
        "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, max_depth=3, learning_rate=0.1)\n",
        "\n",
        "# Fit the model on the training data\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = xgb_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model's performance\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"R^2 Score:\", r2)"
      ],
      "metadata": {
        "id": "PM-DYIg3S7Uh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Binary classification"
      ],
      "metadata": {
        "id": "ikyGaFmnUx91"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "import xgboost as xgb\n",
        "\n",
        "# Load the dataset\n",
        "breast_cancer = load_breast_cancer()\n",
        "X = breast_cancer.data\n",
        "y = breast_cancer.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create an instance of XGBClassifier\n",
        "xgb_model = xgb.XGBClassifier(n_estimators=100, max_depth=3, learning_rate=0.1)\n",
        "\n",
        "# Fit the model on the training data\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = xgb_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model's performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "classification_rep = classification_report(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Confusion Matrix:\\n\", confusion)\n",
        "print(\"Classification Report:\\n\", classification_rep)"
      ],
      "metadata": {
        "id": "3dNcfAPRTBwy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multiclass classification"
      ],
      "metadata": {
        "id": "_GXGxHffU2Al"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "import xgboost as xgb\n",
        "\n",
        "# Load the dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create an instance of XGBClassifier\n",
        "xgb_model = xgb.XGBClassifier(n_estimators=100, max_depth=3, learning_rate=0.1)\n",
        "\n",
        "# Fit the model on the training data\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = xgb_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model's performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "classification_rep = classification_report(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Confusion Matrix:\\n\", confusion)\n",
        "print(\"Classification Report:\\n\", classification_rep)"
      ],
      "metadata": {
        "id": "b97ZWX_eTHHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Ranking"
      ],
      "metadata": {
        "id": "RNiLnyTZU5uZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from sklearn.datasets import load_svmlight_file\n",
        "from sklearn.metrics import ndcg_score\n",
        "\n",
        "# Load the dataset\n",
        "train_data = load_svmlight_file('mq2008.train')\n",
        "X_train, y_train, query_train = train_data[0], train_data[1], train_data[2]\n",
        "\n",
        "test_data = load_svmlight_file('mq2008.test')\n",
        "X_test, y_test, query_test = test_data[0], test_data[1], test_data[2]\n",
        "\n",
        "# Convert query ids to group sizes\n",
        "def query_ids_to_groups(query_ids):\n",
        "    _, group_counts = np.unique(query_ids, return_counts=True)\n",
        "    return group_counts\n",
        "\n",
        "train_groups = query_ids_to_groups(query_train)\n",
        "test_groups = query_ids_to_groups(query_test)\n",
        "\n",
        "# Create an instance of XGBRanker\n",
        "xgb_ranker = xgb.XGBRanker(objective='rank:pairwise', n_estimators=100, max_depth=3, learning_rate=0.1)\n",
        "\n",
        "# Fit the model on the training data\n",
        "xgb_ranker.fit(X_train, y_train, group=train_groups)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = xgb_ranker.predict(X_test)\n",
        "\n",
        "# Evaluate the model's performance\n",
        "ndcg = ndcg_score(test_groups, y_test, y_pred)\n",
        "print(\"Normalized Discounted Cumulative Gain (NDCG):\", ndcg)"
      ],
      "metadata": {
        "id": "DBynyizLTLxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature selection"
      ],
      "metadata": {
        "id": "633RxkvuU9TU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import xgboost as xgb\n",
        "\n",
        "# Load the dataset\n",
        "breast_cancer = load_breast_cancer()\n",
        "X = breast_cancer.data\n",
        "y = breast_cancer.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create an instance of XGBClassifier\n",
        "xgb_model = xgb.XGBClassifier(n_estimators=100, max_depth=3, learning_rate=0.1)\n",
        "\n",
        "# Fit the model on the training data\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importances\n",
        "feature_importances = xgb_model.feature_importances_\n",
        "\n",
        "# Display feature importances\n",
        "for i, importance in enumerate(feature_importances):\n",
        "    print(f\"Feature {i}: {importance}\")\n",
        "\n",
        "# Select the most important features (example: top 5)\n",
        "top_n = 5\n",
        "most_important_indices = np.argsort(feature_importances)[-top_n:]\n",
        "X_train_selected = X_train[:, most_important_indices]\n",
        "X_test_selected = X_test[:, most_important_indices]"
      ],
      "metadata": {
        "id": "q5b_776vTQhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imbalanced datasets"
      ],
      "metadata": {
        "id": "k2Zz3gdgVA85"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "import xgboost as xgb\n",
        "\n",
        "# Generate a synthetic imbalanced dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=2,\n",
        "                           n_redundant=10, n_clusters_per_class=1,\n",
        "                           weights=[0.99], flip_y=0, random_state=42)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Calculate the scale_pos_weight\n",
        "positive_class = np.sum(y_train == 1)\n",
        "negative_class = np.sum(y_train == 0)\n",
        "scale_pos_weight = negative_class / positive_class\n",
        "\n",
        "# Create an instance of XGBClassifier with scale_pos_weight\n",
        "xgb_model = xgb.XGBClassifier(n_estimators=100, max_depth=3, learning_rate=0.1, scale_pos_weight=scale_pos_weight)\n",
        "\n",
        "# Fit the model on the training data\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = xgb_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model's performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "classification_rep = classification_report(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Confusion Matrix:\\n\", confusion)\n",
        "print(\"Classification Report:\\n\", classification_rep)"
      ],
      "metadata": {
        "id": "AN6J--QqTU0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensemble learning"
      ],
      "metadata": {
        "id": "atiJBh3gVEVh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import xgboost as xgb\n",
        "\n",
        "# Load the dataset\n",
        "breast_cancer = load_breast_cancer()\n",
        "X = breast_cancer.data\n",
        "y = breast_cancer.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create instances of the classifiers\n",
        "xgb_model = xgb.XGBClassifier(n_estimators=100, max_depth=3, learning_rate=0.1)\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "logistic_model = LogisticRegression(solver='lbfgs', max_iter=1000, random_state=42)\n",
        "\n",
        "# Create an ensemble using VotingClassifier\n",
        "ensemble_model = VotingClassifier(estimators=[\n",
        "    ('xgb', xgb_model),\n",
        "    ('rf', rf_model),\n",
        "    ('logistic', logistic_model)],\n",
        "    voting='soft')\n",
        "\n",
        "# Fit the ensemble model on the training data\n",
        "ensemble_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = ensemble_model.predict(X_test)\n",
        "\n",
        "# Evaluate the ensemble model's performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "classification_rep = classification_report(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Confusion Matrix:\\n\", confusion)\n",
        "print(\"Classification Report:\\n\", classification_rep)"
      ],
      "metadata": {
        "id": "nvSm5mpVTZmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Time series forecasting"
      ],
      "metadata": {
        "id": "fi6vG47GVIE9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the dataset\n",
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/airline-passengers.csv\"\n",
        "data = pd.read_csv(url)\n",
        "data['Month'] = pd.to_datetime(data['Month'])\n",
        "\n",
        "# Create lag features\n",
        "def create_lag_features(df, n_lags):\n",
        "    for i in range(1, n_lags + 1):\n",
        "        df[f'lag_{i}'] = df['Passengers'].shift(i)\n",
        "    return df\n",
        "\n",
        "n_lags = 3\n",
        "data = create_lag_features(data, n_lags)\n",
        "\n",
        "# Drop rows with NaN values\n",
        "data = data.dropna()\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "train_data = data[data['Month'] < '1958-01-01']\n",
        "test_data = data[data['Month'] >= '1958-01-01']\n",
        "\n",
        "X_train = train_data.drop(['Month', 'Passengers'], axis=1)\n",
        "y_train = train_data['Passengers']\n",
        "\n",
        "X_test = test_data.drop(['Month', 'Passengers'], axis=1)\n",
        "y_test = test_data['Passengers']\n",
        "\n",
        "# Create an instance of XGBRegressor\n",
        "xgb_model = xgb.XGBRegressor(n_estimators=100, max_depth=3, learning_rate=0.1)\n",
        "\n",
        "# Fit the model on the training data\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = xgb_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model's performance\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "print(\"RMSE:\", rmse)\n",
        "\n",
        "# Plot the actual vs. predicted values\n",
        "plt.plot(test_data['Month'], y_test, label=\"Actual\")\n",
        "plt.plot(test_data['Month'], y_pred, label=\"Predicted\")\n",
        "plt.xlabel(\"Month\")\n",
        "plt.ylabel(\"Passengers\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fvBWT4XFTeUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ".Complete Use Case: Predicting House Prices"
      ],
      "metadata": {
        "id": "HtsfwTwfVM2C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the dataset\n",
        "boston = load_boston()\n",
        "X = boston.data\n",
        "y = boston.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create an instance of XGBRegressor\n",
        "xgb_model = xgb.XGBRegressor()\n",
        "\n",
        "# Fit the model on the training data\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = xgb_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model's performance\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Mean Squared Error:\", mse)"
      ],
      "metadata": {
        "id": "8B-L93cmTilh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}